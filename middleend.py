#!/usr/bin/python3

#
#  Weekybot v0.1 middle-end
#
#  Copyright William Hales 2014
#

#  This file is part of Weekybot
#
#  Weekybot is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  Weekybot is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with Weekybot.  If not, see <http://www.gnu.org/licenses/>.



# Usage:
#	./middleend.py             (normal: loop forever)
#	./middleend.py once        (to run only once through)



#### Terminology
# contrib = any type of page edit, delete, create, file upload, etc, 
# generated by feedparser
# action = processed form of a contrib

import re, feedparser, datetime, time, sys

sleep_period = 60  # How often to report, in minutes

feed_url = "http://www.unvanquished.net/wiki/index.php?title=Special:RecentChanges&feed=atom"

#### Contrib type matching
# Contrib type is found in each atom entrie's "summary", but this is also
# in fugly xml/html with the WHOLE change summary included.
# Regexps have extra taggy bits to avoid false positives
contrib_types = []
contrib_types.append( ["uploaded &quot;", "upload"] )
contrib_types.append( ["Older revision</td", "edit"] )
contrib_types.append( ["</a> deleted page <a", "delete"] )
contrib_types.append( ["<p><b>New page</b></p>", "create"] )
contrib_types.append( ["protected &quot;", "protect"] )
contrib_types.append( ["</a> uploaded a new version of &quot;", "new version"] )
contrib_types.append( ["</a> moved page <a class=", "move"] )

#### Colours
# See http://www.ircbeginner.com/ircinfo/colors.html for codes
# All prefixed with \x03 to simulate control+c
colour_header =  "\x034" # Red
colour_pagetitle = "" # Nil
colour_contribs = "\x0315" # Light Gray


#### Other
runonce = False
if len( sys.argv ) > 1:
	if sys.argv[1] == "once":
		runonce = True

def unique(seq):  # From http://www.peterbe.com/plog/uniqifiers-benchmark
   checked = []
   for e in seq:
       if e not in checked:
           checked.append(e)
   return checked


#### Main loop
running = True
# Unvanquished wiki uses UTC
time_target = datetime.datetime.utcnow()
time_last = time_target - datetime.timedelta(minutes=sleep_period)

while running:
	
	time_to_wait = time.mktime(time_target.timetuple()) - time.mktime(datetime.datetime.utcnow().timetuple()) # Done in epoch seconds
	if time_to_wait > 0:
		time.sleep( time_to_wait )
	# It should now be approximately time_target
	
	
	feed = feedparser.parse( feed_url )

	contribs = []
	
	for entry_num in range( len(feed.entries) ):
		contrib = { 'title':'unknown', 'type':'unknown', 'author':'unknown', 'processed_flag': False }
		if datetime.datetime.strptime(feed.entries[entry_num].date,"%Y-%m-%dT%H:%M:%SZ") < time_last:
			continue # Too old
		else:
			contrib['title'] =  feed.entries[entry_num].title
			contrib['author'] = feed.entries[entry_num].author
			contrib['type'] = 'unknown'
			summary = feed.entries[entry_num].summary[0:2000] # Length optim not neccesary?
			for contype in contrib_types:
				if re.search( contype[0], summary):
					contrib['type'] = contype[1]
					break
			contribs.append( contrib)
			
		
	
	
	length = len(contribs)

	pages = []
	# Example structure.  name[] = list, name = dictionary, "name" = val
	
	#	pages[]
	#		page
	#			title:"Main Page"
	#			actions
	#				edit[]
	#					"jamaal"
	#					"veyrdite"
	#					"veyrdite"
	#					"veyrdite"
	#					"jamaal"
	#					"viech"
	#				delete[] 
	#					"petrified acorns"
	#				revert[]
	#					"granger"
	
	# Structure optimised for using 'for' to ieterate through contents
	# We fix duplicate authors later

	if length != 0:
		plural=""
		if length > 1:
			plural="s"
		
		print( colour_header + "- Wiki:", str(length), "contribution" + plural + " made in the last", sleep_period, "mins -")	
		
		# Group contribs by (1) page and (2) action		
		for contrib in contribs:
			if contrib['processed_flag'] == False:
				page = {'title':contrib['title'], 'actions':{} }

				for contrib_type in contrib_types:
					page['actions'][contrib_type[1]] = [] 
				page['actions']['unknown'] = [] # Used when type unknown
					
				contrib['processed_flag'] = True
				
				# This first action is guaranteed to be unique
				page['actions'][contrib['type']].append( contrib['author'] )
				
				# Cycle through all contribs to find similiar ones
				# This algorith is easier to understand than a reducing list
				
				for contrib2 in contribs:
					if contrib2['processed_flag'] == False and contrib2['title'] == page['title']:
						# Same page being affected as we're after
						contrib2['processed_flag'] = True
						page['actions'][contrib2['type']].append( contrib2['author'] )
				
				pages.append( page)
				

		# Final output
		for page in pages:
			string = colour_pagetitle + page['title'] + colour_contribs + ' - '
			
			for action in page['actions']:
				if len(page['actions'][action]) > 0:
					string = string + action + ' ('
					
					# Add author names
					first = True
					for author in unique(page['actions'][action]): # unique'd
						if first == False:
							string = string + ' '
						else:
							first = False
						string = string + author
					string = string + ') '
			
			print( string)
							
	
						
	
	sys.stdout.flush()
	time_last = time_target
	time_target = time_target + datetime.timedelta(minutes=sleep_period)
	
	if runonce == True:
		running = False





